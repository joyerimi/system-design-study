# 웹 크롤러 설계
웹 크롤러는 검색 엔진에서 쓰이는 기술로, 웹에 새로 올라오는 콘텐츠를 찾아내는 것이 주된 목적입니다.
콘텐츠는 웹 페이지, 이미지, 비디오, PDF파일일 수 있습니다.
몇 개 웹 페이지에서 시작하여 링크를 따라가며 콘텐츠를 수집합니다.  
크롤러가 사용되는 목적은 다음과 같습니다.
1. 검색 엔진 인덱싱 : 크롤러의 가장 보편적인 용례.
  웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
2. 웹 아카이빙 : 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차이다.
3. 웹 마이닝 : 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해낸다.
4. 웹 모니터링 : 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링한다.

웹 크롤러의 복잡도에 따라 몇 시간이면 끝날 수 있거나 지속적으로 관리하고 개선해야 하는 프로젝트가 될 수 있습니다.

## 1단계 : 문제 이해 및 설계 범위 확정
웹 크롤링의 알고리즘은 간단합니다.
1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

하지만 웹 크롤러는 이렇게 순순히 작동하지 않습니다. 규모 확장성을 갖는 웹 크롤러를 설계하기는 어렵습니다.(진짜임)

역시 질문을 통해 요구사항 파악 및 설계 범위를 좁히는 것부터 시작합니다.
웹 크롤러를 설계할 때 다음과 같은 점을 고려해야 합니다.

- 규모 확장성 : 웹은 거대하므로 병행성을 활용한다면 효과적으로 크롤링할 수 있다.
- 안정성 : 크롤링할 대상에 문제가 많다. 잘못 작성된 HTML, 반응 없는 서버, 장애, 악성코드가 붙은 링크 등
  비정상적 입력이나 환경에 잘 대응해야 한다.
- 예절 : 크롤러는 수집 대상 웹사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안된다.
- 확장성 : 새로운 형태의 콘텐츠를 지원하기가 쉬워야 한다. HTML만 크롤링하다가 이미지 파일도 하고싶을 때
  전체 시스템을 새로 설계해야 한다면 곤란하다.

### 개략적 규모 추정
질문으로부터 얻어낸 예시는 다음과 같습니다.
- 매달 10억 개의 웹 페이지 다운로드
  - QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 페이지/초
- 최대 QPS = 2 * QPS = 800
- 웹 페이지의 평균 크기 500K
- 10억 페이지 * 500k = 500TB/월
- 5년치 데이터 = 500TB * 12 * 5 = 30PB

## 2단계 개략적 설계안 제시 및 동의 구하기

요구사항을 만족하는 개략적 설계를 진행하겠습니다. 먼저 어떤 컴포넌트가 있는지부터 알아봅시다.

<img width="661" height="473" alt="Image" src="https://github.com/user-attachments/assets/c47bbbb5-956a-4aed-b269-bb80b05036a7" />

### 시작 URL 집합
시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점입니다.
시작 URL을 잘 설정해야 불필요한 탐색을 줄이고 효율적인 탐색이 가능합니다.
어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹사이트를 크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은
모든 페이지의 URL을 시작 URL로 쓰는 것입니다.
무엇을 쓸 것인지 정답은 없으니, 목적에 알맞게만 의도를 전달하도록 합시다.

### 미수집 URL 저장소
FIFO Queue 구조로, 다운로드할 예정인 URL을 모아놓는 곳입니다.

### HTML 다운로더
인터넷에서 웹 페이지를 다운로드하는 컴포넌트입니다.

### 도메인 이름 변환기
웹 페이지를 다운받기 위해서는 URL을 IP 주소로 변환하는 절차가 필요합니다.
이 때 변환기를 사용하여 URL에 대응하는 IP 주소를 알아냅니다.

### 콘텐츠 파서
웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 합니다. 이상한 웹 페이지는 문제를 일으키고 저장 공간 낭비만 됩니다.
크롤링 서버에 만들게 되면 크롤링 과정이 느려지게 될 수 있으므로 독립된 컴포넌트로 만듭니다.

### 중복 콘텐츠?
웹의 29% 가량의 웹 페이지 콘텐츠는 중복입니다.
> 이 책 단어 몇 개 검색하면 다 똑같은 내용나오는 거보면 29% 이상 같습니다.

중복된 데이터를 저장하지 않기 위해 자료 구조를 도입하고, 데이터 처리에 소요되는 시간을 줄입니다.
모든 문자열을 비교하면 너무 느리니 웹 페이지의 해시 값을 비교하는 것이 효과적입니다.

> 한 글자만 바뀌어도 해시 값이 바뀌는데, HTML이 완전 동일할까?

### 콘텐츠 저장소

HTML 문서를 보관하는 시스템입니다.
저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간등을 종합적으로 고려해야 합니다.
예시 설계안은 디스크와 메모리를 동시에 사용하는 저장소를 택할 것입니다.

### URL 추출기
HTML 페이지를 파싱하여 링크들을 골라내는 역할을 합니다.
상대 경로의 경우 전부 절대 경로로 변환하여 반환해야 합니다.

### URL 필터
특정한 컨텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL 등
크롤링 대상에서 배제하는 역할을 합니다.

### 이미 방문한 URL?
이미 방문한 적 있는 URL인지 판단할 수 있으면 서버 부하를 줄이고 무한 루프에 빠지는 일을 방지 할 수 있습니다.
보통 블룸 필터나 해시 테이블을 사용하여 방문 여부를 판단합니다.

### URL 저장소
이미 방문한 URL을 보관하는 저장소입니다.

작업 흐름은 작성한 순서대로 진행되고 특별한 점이 없어 추가로 작성하지 않았습니다.

## 3단계 : 상세 설계

지금까지 웹 크롤러를 구성하는 컴포넌트를 알아보았습니다.
이제 컴포넌트의 기능을 구현하는 데 사용하는 구현 기술을 알아보겠습니다

### DFS vs BFS
웹은 유향 그래프나 다름없습니다. 페이지를 노드, URL을 에지라고 보면 그렇지 않습니까?
탐색하고자 하는 범위의 깊이를 가늠하기 어려우므로 좋은 선택이 아닐 가능성이 높습니다.
따라서 보통은 BFS를 사용하여 구현하게 되는데, BFS에도 문제점은 있습니다.
- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다. 
  같은 서버로 되돌아가는 요청을 처리하다 보면 서버에 과부하가 걸리게 된다. 
- 표준적 BFS 알고리즘은 URL 간 우선순위가 없다. 처리 순서에 있어 모든 페이지를 공평하게 대우한다.
  페이지의 품질, 중요성을 고려하지 않으므로 우선순위를 구별하는 것이 좋은 선택일 것이다.

BFS에서 발생할 수 있는 2가지 문제점은 미수집 URL 저장소를 활용하여 해결할 수 있습니다.

### 미수집 URL 저장소
#### 예의
웹 크롤러는 단시간에 수집 대상 서버로 많은 요청을 보내는 것을 삼가야 합니다.
따라서 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청합니다.
웹사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지하면 됩니다.
즉, 각 다운로드 스레드는 별도 FIFO 큐를 가지고 있어, 해당 큐에서 꺼낸 URL만 다운로드합니다.

<img width="607" height="524" alt="Image" src="https://github.com/user-attachments/assets/d11875a9-2dc9-4856-895b-9d8d778bd428" />

- 큐 라우터 : 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할
- 매핑 테이블 : 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- FIFO 큐 : 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
- 큐 선택기 : 큐 선택기는 큐들을 순회하면서 큐에서 URL을 꺼내 해당 큐에서 나온 URL을 다운로드 하도록 지정된 작업 스레드에 전달하는 역할
- 작업 스레드 : 전달된 URL을 다운로드하는 작업 수행

해당 구조를 사용하게 되면 수집 대상 서버로 단시간 과부하를 예방할 수 있습니다.

#### 우선순위
크롤러 입장에서는 중요한 페이지가 먼저 수집되는 것이 바람직합니다.
URL의 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있습니다.

<img width="600" height="450" alt="Image" src="https://github.com/user-attachments/assets/57ffda26-40f0-42a6-b09a-e6f269fdc6f1" />

앞에서 봤던 이미지의 큐 라우터 앞에 순위결정장치를 배치합니다.
- 순위결정장치 : URL을 받아 우선순위를 계산
- 큐 : 우선순위별로 큐가 하나씩 할당된다. 우선순위가 높을수록 선택될 확률도 높아진다.
- 큐 선택기 : 임의 큐에서 처리할 URL을 꺼내는 역할. 순위가 높은 큐에서 더 자주 꺼내도록 설계된다.

#### 신선도
웹 페이지는 수시로 추가되고, 삭제되고, 변경됩니다.
따라서 데이터의 신선함을 유지하기 위해 이미 다운로드한 페이지더라도 주기적으로 재수집할 필요가 있습니다.
모든 페이지를 재수집하는 것은 많은 비용이 들어가므로 웹 페이지 변경 이력을 활용하거나 우선순위를 활용하여 중요한 페이지는 자주 재수집하는 방법을 사용 가능합니다.

#### 미수집 URL 저장소를 위한 지속성 저장 장치
검색 엔진을 위한 크롤러의 경우 처리해야 하는 URL의 수는 수억 개입니다.
따라서 모든 URL을 메모리에 보관하는 것은 안정성이나 규모 확장성 측면에서 바람직하지 않습니다.
그렇다고 디스크에 모두 넣어놓으면 느린 I/O 시간으로 인해 성능 병목지점이 될 수 있습니다.

따라서 절충안으로 선택한 것이 메모리 버퍼에 큐를 두는 것입니다.

### HTML 다운로더
HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려받습니다.
알아보기 전에 로봇 제외 프로토콜에 대해 알아보고 가겠습니다.

#### Robots.txt
웹사이트와 크롤러가 소통하는 표준적 방법입니다.
크롤러가 어떤 URL을 수집해도 되는지 나열되어 있습니다.

#### 성능 최적화
HTML 다운로더에 적용가능한 최적화 기법들을 소개합니다.

1. 분산 크롤링  
  성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법입니다.
  
  <img width="427" height="328" alt="Image" src="https://github.com/user-attachments/assets/301f466f-fcc9-464f-9950-5145292003b8" />

2. 도메인 이름 변환 결과 캐시  
  도메인 이름 변환기는 크롤러 성능의 병목 중 하나인데, 이는 DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 때문입니다.
  DNS 요청의 경우 보통 10ms ~ 200ms가 소요되므로, DNS 조회 결과로 얻은 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해놓고 크론 잡 등을 돌려 주기적으로 갱신하도록 해놓으면 성능을 높일 수 있습니다.
3. 지역성  
  크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법입니다.
  크롤링 서버와 수집 대상 서버가 같은 지역에 있다면 다운로드 시간이 줄어들 것입니다.
4. 짧은 타임아웃  
  어떤 웹 서버는 응답이 느리거나 응답하지 않을 수 있습니다. 대기 시간이 길어지면 좋지 않으므로 최대 대기 시간을 설정해놓으면 문제를 사전에 방지할 수 있습니다.

#### 안정성
최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려해야 합니다.
- 안정 해시 : 다운로더 서버들에 부하를 분산할 때 적용 가능
- 크롤링 상태 및 수집 데이터 저장 : 장애가 발생한 경우 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해두는 것이 좋습니다.
- 예외 처리 : 대규모 시스템에서 에러는 피할 수 없으므로 처리 필요
- 데이터 검증 : 시스템 오류를 방지하기 위한 중요 수단

#### 확장성
새로운 형태의 콘텐츠츨 쉽게 지원할 수 있도록 신경써야 합니다.

<img width="811" height="454" alt="Image" src="https://github.com/user-attachments/assets/ad7856f8-3b54-44d9-8080-4bb1e6aef7e4" />

다양한 모듈을 사용하여 여러 콘텐츠를 크롤링할 수 있도록 만들 수 있습니다.

> 웹 모니터 : 웹을 모니터링하여 저작권이나 상표권이 침해되는 일을 막는 모듈

#### 문제 있는 콘텐츠 감지 및 회피

1. 중복 컨텐츠 : 해시나 체크섬을 이용하여 중복된 컨텐츠는 거를 수 있도록 한다.
2. 거미 덫 : 크롤러를 무한 루프에 빠지게 설계한 웹 페이지이다.  
  모든 덫을 피할 수 있는 만능 해결책이 없으므로 수작업으로 덫을 확인하고 제외하는 방법을 쓰거나 한다.
3. 데이터 노이즈 : 가치가 없는 데이터를 크롤링하지 않도록 조심해야 한다.

## 4단계 : 마무리
좋은 크롤러가 갖춰야 하는 특징들에 대해 살펴보았습니다.
규모 확장성, 예의, 확장성, 안정성 등 있었습니다.
추가적으로 논의해보면 좋을 내용은 다음과 같습니다.

### Server-Side Rendering
요즘에는 HTML을 다운받아 크롤링하는 정적 크롤링에는 한계가 있습니다.
동적으로 생성되는 링크는 발견할 수 없고, 컨텐츠를 복사하기 어렵기 때문입니다.
동적 렌더링 방식을 사용한다면 해결할 수 있을 것입니다.

### 원치 않는 페이지 필터링
저장 공간 등 크롤링에 소요되는 자원은 유한하므로 스팸 방지 컴포넌트를 두어
품질이 좋지 않거나 스팸 페이지는 거를 수 있도록 해야 합니다.

### 수평적 규모 확장성
서버를 stateless로 구현해야 할 것입니다.
대규모 크롤링의 경우 서버가 수백 수천 대가 필요할 수 있기 때문입니다.

### 가용성, 일관성, 안정성
대규모 시스템에서는 빠질 수 없는 친구들입니다.

### 데이터 분석 솔루션
시스템을 세밀히 조정하기 위해서는 데이터를 분석하여 결과를 활용해야 할 것입니다.


# 크롤링 기능 제작 후기
공통 프로젝트를 진행하면서 링크만 있으면 요약 내용을 생성해주는 크롤링 기능을 만들었습니다.
작동 원리, 만들면서 느낀 점을 공유하도록 하겠습니다.

### 작동 원리

<img width="968" height="340" alt="Image" src="https://github.com/user-attachments/assets/303b265b-dc66-4513-bd15-b03c77be136b" />

1. 먼저 페이지의 HTML을 다운로드 받습니다.
2. 다운로드 받은 HTML에서 정적 크롤링 도구인 Jsoup 이용하여 크롤링을 시도합니다.
3. 실패한다면 동적 크롤링 도구인 Selenium을 이용하여 크롤링을 합니다.
4. 크롤링 결과에서 요약 생성에 필요없는 태그(script, footer, header 등)를 제외합니다.
5. 이후에 내용이 가장 긴 article 혹은 div를 찾습니다.
6. 해당 태그 안에 있는 문자열을 HTML 태그를 제거하고 저장합니다.
7. 정제해서 추출한 문자열을 AI에게 Input으로 사용해 요약을 생성합니다.

### 느낀 점
서비스 개발하다보면 조잡하든 복잡하든 크롤링 기능을 한 번쯤은 구현할 것 같습니다.

---

**확장성**을 갖춘 크롤러 개발하기 진짜 어렵습니다.
저도 불특정한 기술 블로그 링크를 입력으로 받고 싶어서 고민을 좀 했었는데, 사이트마다 HTML구조가 너무 달랐습니다.

#### 근데 이런 확장성을 갖추지 않으면 구린 크롤러 아니에요??
사실 저는 책 내용과는 좀 반대되는 입장입니다. 여러 사이트, 여러 컨텐츠에 대해서 고려하지 않아도 된다고 생각합니다.

왜냐하면 생성형 AI가 너무 좋기 때문입니다.
크롤링 하고자하는 사이트의 HTML 구조에 대해 설명하면 해당 페이지를 크롤링하는 코드를 순식간에 만들어줍니다.

물론 대규모 프로젝트거나, 검색 엔진을 위한 크롤러거나 하면 이야기가 달라지겠죠,,,
사이드 프로젝트를 위한 크롤러를 만들 때는 책에서 다루는 수준의 고민은 필요 없는 것 같습니다.

범용성을 갖춘 크롤러 개발하는 것의 어려움을 많이 느꼈습니다.

---

성능 최적화하기가 상당히 힘들었습니다. 특히 셀레니움의 경우 렌더링되는 시간을 기다려야 하니 더 오래 걸렸습니다.

---

서버를 따로 띄우지 못해 좀 아쉬웠습니다.
원래 생각은 API 호출 서버와 AI 요약 서버를 따로 띄우려 했으나 제 역량부족으로 실패했습니다.  
책을 읽으면서 느꼈지만 크롤링이 서버 장애를 많이 일으킬 수 있는 것 같습니다.
이후에는 서버를 따로 띄워 장애 대응이 가능하도록 만들고 싶습니다.

---

법적 문제를 고려하기 참 까다로운 것 같습니다.
저희는 그냥 진행했는데 법률팀없는 회사에서 함부로 쓰다가 잡혀갈 것 같습니다.  
개발자라면 저작권의 법적 문제를 세세히 알기 어려우니 주의해서 사용해야 하지 않을까 싶습니다.

---

#### 파이썬이 크롤링 대세 아님?
맞는 것 같습니다. 근데 Jsoup도 우리 레벨에서는 충분히 좋은듯.  
파이썬을 사용하려면 결국 AI 요청 서버를 하나 더 띄워야 했는데, 단일 서버로 잘 마무리되어서 다행이었습니다.
기술블로그 크롤링은 파이썬, 사용자 요청 블로그 크롤링은 자바로 했습니다.
파이썬이 코드가 더 간결하고 이해하기 편하고 GPT가 더 잘 만들어주는 느낌.

간단한 크롤링을 해서 그런지 성능 차이는 못느꼈습니다.