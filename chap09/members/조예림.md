## 0단계: 개요

> 대규모 크롤러 설계에 들어가기 전, 웹 크롤러의 개념과 활용 사례, 그리고 왜 복잡한 설계가 필요한지를 설명합니다.
> 

### 0.1 웹 크롤러란?

**웹페이지, 이미지, 영상, PDF 등 인터넷 상의 콘텐츠를 자동으로 탐색하고 수집하는 프로그램**입니다.

### 기본 작동 방식

![image.png](https://github.com/user-attachments/assets/269920e8-b4fd-45dc-9873-ded020a83f4d)

1. 몇 개의 시작 URL에서 출발
2. 해당 웹페이지를 다운로드
3. HTML을 파싱해 새 링크(URL)를 추출
4. 추출된 URL을 다시 크롤링 대상으로 큐에 추가
5. 위 과정을 반복하며 웹을 순차적으로 탐색

### 0.2 웹 크롤러의 활용 분야

웹 크롤러는 다양한 분야에서 광범위하게 활용됩니다. 목적에 따라 수집 대상, 주기, 처리 방식이 달라집니다.

| 활용 분야 | 설명 | 실제 사례 |
| --- | --- | --- |
| **검색 인덱싱** | 웹페이지를 수집해 검색용 색인을 구축 | Googlebot |
| **웹 아카이빙** | 장기 보관을 위해 웹사이트 전체를 저장 | 미국 국회도서관, EU 웹 아카이브 |
| **웹 마이닝** | 수집한 데이터를 분석해 인사이트 도출 | 금융 기업의 연차보고서 분석 |
| **웹 모니터링** | 저작권·상표권 침해 감지, 브랜드 보호 | Digimarc (해적판 탐지) |

> 크롤러의 용도에 따라 설계 시 고려해야 할 요구사항이 완전히 달라질 수 있습니다.
따라서 먼저 **크롤링 목표와 요구사항**을 명확히 해야 합니다.
> 
> 
> 예: 수집 범위, 주기, 실시간성 등
> 

---

## 1단계: 문제 이해 및 설계 범위 확정

### 1.1 기본 크롤링 알고리즘

1. URL 목록을 입력받는다.
2. 각 URL을 다운로드하여 웹페이지를 가져온다.
3. 페이지에서 새로운 URL을 추출한다.
4. 추출한 URL들을 다시 다운로드 대상으로 추가하고 반복한다.

> 겉보기엔 단순하지만, 대규모 확장성을 갖춘 크롤러 설계는 복잡하고 어렵다.
> 
> 
> 따라서 먼저 **요구사항을 명확히 파악해 설계 범위를 좁히는 게 핵심**이다.
> 

### 1.2 요구사항 도출

| 질문 | 답변 |
| --- | --- |
| 크롤러의 용도는? | 검색 엔진 인덱싱용 |
| 수집해야 할 웹페이지 수는? | 월 10억 개 |
| 신규/수정된 페이지도 수집? | 예 |
| 수집한 데이터는 저장해야 하나요? | 예, 5년간 보관 |
| 중복 콘텐츠는 어떻게 하나요? | 무시해도 됨 |

### 1.3 필수 속성

| 속성 | 설명 |
| --- | --- |
| **확장성** | 수십억 페이지를 빠르게 처리하려면 **병렬 처리**가 필요 |
| **안정성** | 오류 페이지, 악성코드, 깨진 링크 등 비정상 상황에 잘 대응 |
| **예절성** | 특정 사이트에 과도한 요청을 보내지 않도록 제한 필요 |
| **확장성** | 이미지, PDF 등 새로운 형식의 콘텐츠도 유연하게 지원 가능 |

### 1.4 대략적인 시스템 규모 추정

| 항목 | 값 |
| --- | --- |
| 월간 크롤링 대상 | 10억 페이지 |
| 평균 페이지 크기 | 500KB |
| 월 저장 용량 | 500TB |
| 5년 누적 저장 용량 | 30PB |
| 평균 QPS (초당 요청 수) | 약 400 QPS |
| 최대 피크 QPS | 약 800 QPS |

> QPS 계산식: 10억 / 30일 / 24시간 / 3600초 ≈ 400
> 

---

## 2단계: 개략적 설계안 제시 및 동의 구하기

### 2.1 전체 아키텍처 구성

![image.png](https://github.com/user-attachments/assets/6d4e41ff-dd0f-4a87-8a4d-92ee4eb78d14)

### 작업 흐름 요약

① 시작 URL들을 미수집 URL 저장소에 저장한다.

② HTML 다운로더가 미수집 URL 저장소에서 URL을 꺼낸다.

③ 다운로더는 도메인 이름 변환기를 통해 IP 주소를 알아내고, 해당 서버에 접속해 HTML 페이지를 다운로드한다.

④ 콘텐츠 파서가 HTML을 파싱하고 유효한 형식인지 검사한다.

⑤ 유효하면 중복 콘텐츠 여부를 검사한다.

⑥ 이미 저장된 콘텐츠라면 버리고, 저장되지 않은 콘텐츠라면 저장소에 저장한 후 URL 추출기로 넘긴다.

⑦ URL 추출기가 HTML에서 새로운 링크들을 추출한다.

⑧ 추출된 링크들을 URL 필터에 전달해 불필요한 URL을 제거한다.

⑨ 필터링된 URL들을 중복 URL 판별 단계로 보낸다.

⑩ 각 URL이 이미 방문한 URL 저장소에 있는지 확인하고, 있으면 버린다.

⑪ 방문한 적 없는 URL은 URL 저장소와 미수집 URL 저장소에 모두 저장한다.

### 2.2 각 구성 요소 설명

### 2.2.1 시작 URL 집합

- **역할**: 크롤링의 출발점이 되는 초기 URL 목록
- **왜 필요한가?**: 웹은 유한하지 않으므로 출발점을 명확히 정해야 확장 탐색이 가능함
- **상호작용**: 최초 입력 → 미수집 URL 저장소로 전달됨
- **기술적 고려사항**
    - 특정 사이트 대상일 경우 → 해당 도메인의 대표 URL 1~2개 선정
    - 전체 웹 크롤링일 경우 → 인기 사이트, 나라별 대표 사이트, 주제별(뉴스, 쇼핑 등) 사이트 활용
    - 더 많은 링크로 연결된 URL을 선택하면 → 탐색 범위가 빠르게 확장됨

### 2.2.2 미수집 URL 저장소

- **역할**: 아직 방문하지 않은 URL들을 저장하는 큐
- **왜 필요한가?**: 크롤링 순서를 제어하고, 재방문을 방지하기 위해
- **상호작용**
    - 시작 URL 집합 → 저장
    - 다운로더 ← URL 꺼내기
    - URL 추출기/중복 판별기 → 새 URL 추가
- **기술적 고려사항**
    - 일반적으로 FIFO 방식
    - 우선순위 큐·도메인 큐 등으로 확장 가능

### 2.2.3 HTML 다운로더

- **역할**: 지정된 URL의 웹페이지(HTML)를 실제로 다운로드
- **왜 필요한가?**: 웹과의 연결 지점이며 크롤링 성능에 직접 영향
- **상호작용**
    - 미수집 URL 저장소에서 URL 받아옴
    - DNS 조회 후 HTTP 요청 수행
- **기술적 고려사항**
    - robots.txt 확인 필요
    - 타임아웃, 리트라이 정책
    - 병렬 처리 지원

### 2.2.4 도메인 이름 변환기 (DNS)

- **역할**: URL에 포함된 도메인 이름을 IP 주소로 변환
- **왜 필요한가?**: 실제 웹서버에 접근하려면 IP 주소가 필요
- **상호작용**: 다운로더 ↔ DNS → 웹서버 접속
- **기술적 고려사항**
    - DNS 캐시로 반복 조회 줄이기
    - 지연 최소화를 위한 로컬 DNS 서버 활용

### 2.2.5 콘텐츠 파서

- **역할**: 다운로드한 HTML을 파싱하고 유효성 검사를 수행
- **왜 필요한가?**: 오류 페이지·비정상 페이지를 걸러내고, 유효한 콘텐츠만 추출
- **상호작용**: 다운로더 → 콘텐츠 파서 → 중복 판별기
- **기술적 고려사항**
    - HTML 파서 성능 및 정확도
    - 비정형 HTML 처리 가능성 고려
    - 병렬 파싱 필요

### 2.2.6 중복 콘텐츠 판별기

- **역할**: 이미 저장된 콘텐츠와 동일한지 판단
- **왜 필요한가?**: 저장 공간 절약, 불필요한 데이터 처리 방지
- **상호작용**: 콘텐츠 파서 → 판별기 → 콘텐츠 저장소
- **기술적 고려사항**
    - 해시값(SHA256 등) 기반 비교
    - 퍼지 해싱(Fuzzy Hash) 활용 가능
    - 전체 비교 대신 부분 비교 고려

### 2.2.7 콘텐츠 저장소

- **역할**: 수집된 유효한 웹페이지 데이터를 저장
- **왜 필요한가?**: 색인, 분석, 검색 등을 위한 영속적 데이터 보관
- **상호작용**: 중복 판별기 → 저장소, 검색엔진 등과 연결 가능
- **기술적 고려사항**
    - **자주 접근되는 콘텐츠**는 메모리에 저장
    - 분산 파일 시스템(HDFS 등) 고려
    - 압축 저장, 버전 관리 가능성

### 2.2.8 URL 추출기

![image.png](https://github.com/user-attachments/assets/bd12d9d4-b881-4645-b48a-4546bb5fa502)

- **역할**: HTML 내부에서 새로운 링크(URL)를 파싱해 추출
- **왜 필요한가?**: 크롤링 대상을 계속 확장하기 위해
- **상호작용**: 콘텐츠 파서 → URL 추출기 → URL 필터
- **기술적 고려사항**
    - 상대경로 → 절대경로 변환
    - 자바스크립트 기반 동적 링크는 별도 처리 필요

### 2.2.9 URL 필터

- **역할**: 크롤링 대상에서 제외할 URL을 사전에 걸러냄
- **왜 필요한가?**: 불필요한 리소스 낭비 방지, 안정성 확보
- **상호작용**: URL 추출기 → URL 필터 → 중복 판별기
- **기술적 고려사항**
    - 확장자 기반 필터링 (`.exe`, `.zip`)
    - robots.txt 및 블랙리스트 적용
    - 패턴 기반 정규식 필터링

### 2.2.10 방문 이력 확인 및 URL 저장소

- **역할**: 이미 방문한 URL인지 확인하고 기록
- **왜 필요한가?**: 중복 크롤링 방지, 무한 루프 차단
- **상호작용**: URL 필터 → 중복 확인 → 미수집 URL 저장소 or 버림
- **기술적 고려사항**:
    - 블룸 필터, 해시셋 등 공간 효율적 자료구조
    - 대용량 처리 위한 샤딩 또는 파티셔닝
    - 디스크+메모리 하이브리드 저장 고려

---

## 3단계: 상세 설계

### 3.1 크롤링 알고리즘 전략

### DFS vs BFS 비교

| 알고리즘 | 설명 | 웹 크롤러에 적합한가? |
| --- | --- | --- |
| **DFS** | 한 링크를 따라 깊이 들어간 후 되돌아오는 방식 | 비추천 – 너무 깊이 들어가 탐색 범위가 비효율적 |
| **BFS** | 한 단계의 모든 링크를 먼저 탐색 후 다음 단계로 넘어감 | 일반적으로 사용 – 탐색이 균형 있고 예측 가능 |
- 웹은 **방향성 그래프**로 표현할 수 있음
    
    → 페이지 = 노드, 링크 = 엣지
    
    → 크롤링 = 그래프 탐색
    

### BFS 사용 시 주의점

**문제 1: 같은 도메인에 과도한 요청**

- 예: `wikipedia.com`의 대부분 링크가 내부 페이지로 연결됨
- BFS 방식으로 크롤링하면 → 특정 도메인에 요청이 집중됨
- 동시에 여러 스레드가 요청할 경우 → **서버 과부하 발생**
    
    → 예의 없는 크롤러로 인식될 수 있음
    

![image.png](https://github.com/user-attachments/assets/c039151e-4b3a-4948-b83f-993aa6f55354)

**문제 2: 우선순위 없는 일괄 처리**

- 기본 BFS는 **모든 페이지를 동일하게 취급함**
    
    → 실제로는 페이지마다 중요도, 업데이트 주기, 트래픽 등이 다름
    
    - 예시
        - 검색 중요도 (PageRank)
        - 사용자 트래픽
        - 변경 빈도

> 따라서 BFS에 우선순위 큐(priority queue) 개념을 추가하면,
> 
> 
> 더 중요한 URL부터 선별적으로 처리할 수 있어 효율적임
> 

### 3.2 URL 큐 설계 전략

> URL 큐는 단순한 작업 목록이 아니라, 크롤링의 속도·공정성·효율성을 좌우하는 핵심 구성요소입니다.
> 
> 
> 이 섹션에서는 큐 설계에 있어 반드시 고려해야 할 네 가지 측면 — 예절성, 우선순위, 신선도, 저장 전략 — 을 설명합니다.
> 

### 큐 구성 요소 요약

| 요소 | 역할 |
| --- | --- |
| **도메인별 큐** | 동일 호스트 요청을 제어해 예절 유지 |
| **우선순위 큐** | 중요도 높은 URL부터 처리 |
| **신선도 관리** | 페이지 변경 주기에 따라 재방문 |
| **하이브리드 저장** | 디스크 + 메모리로 확장성과 속도 모두 확보 |

### 3.2.1 예절성 (Politeness) 유지

> 같은 도메인에 짧은 시간 내 과도한 요청을 보내면, 서버에 과부하를 줄 수 있습니다.
> 
> 
> 이를 방지하는 메커니즘이 **도메인 기반 큐와 요청 간 딜레이**입니다.
> 

### 해결 전략

- **도메인별 큐 구성**
    
    → 같은 호스트(URL 도메인)는 동일 큐로 분리
    
- **큐 라우터 / 매핑 테이블**
    
    → `hostname → 큐 번호`로 매핑
    
    ![image.png](https://github.com/user-attachments/assets/342ba934-5b9a-4fff-85da-300ddc81503e)
    
- **작업 스레드**는 각 큐만 읽고 요청
    
    → 요청 간에 **딜레이(delay)**를 적용
    
    ![image.png](https://github.com/user-attachments/assets/f6e8881b-651e-43d0-8f51-9058f73cad8b)
    

### 3.2.2 우선순위 처리

> 모든 페이지가 동일하게 중요하지 않습니다.
> 
> 
> 따라서 PageRank, 트래픽 등 기반으로 **더 중요한 URL부터 먼저 처리**하는 구조가 필요합니다.
> 

![image.png](https://github.com/user-attachments/assets/5e13d838-00d9-4728-b5a1-f0e02e42ebed)

### 구성 요소

1. **Prioritizer**: URL의 중요도를 계산
2. **Front Queue**: 우선순위 등급에 따라 URL 분류
3. **Queue Selector**: Front Queue를 순회하며 URL 선택
4. **Back Queue**: 동일 도메인 요청 간 예절성 유지

---

### 3.2.3 신선도 관리 (Freshness)

> 웹 페이지는 수시로 변경되므로, 일정 주기로 재방문이 필요합니다.
> 
> 
> 모든 페이지를 동일 주기로 재크롤링하는 것은 비효율적입니다.
> 

### 해결 전략

- **업데이트 이력 기반 주기 추정**
    
    → `Last-Modified`, `ETag`, 해시값 등을 활용
    
    → 자주 변경되는 페이지는 주기를 단축, 안정적인 페이지는 주기를 늘림
    
- **우선순위 기반 재방문 정책**
    
    → 중요도(PageRank 등)가 높은 페이지는 더 자주 수집
    

---

### 3.2.4 저장소 설계

> 수억 개 URL을 다뤄야 하므로, 저장 성능과 확장성 모두 확보해야 합니다.
> 

### 문제점

- 메모리에만 저장 → 용량 제한, 휘발성
- 디스크에만 저장 → 속도 저하, 병목 발생

### 해결 전략: **하이브리드 저장**

- 대부분 URL → **디스크**에 저장해 확장성 확보
- 자주 사용하는 작업 큐 → **메모리 버퍼**로 빠른 접근
- 메모리 큐는 **주기적으로 디스크에 기록**하여 영속성 유지

### 3.3 HTML 다운로더 고도화

### 3.3.1 Robots.txt 준수

- **역할**: 각 웹사이트가 크롤러에게 제공하는 **접근 허용/비허용 규칙**을 확인하고, 이를 지켜 요청을 제한
- **왜 필요한가?**
    - 서버 운영자가 허용하지 않은 영역까지 수집하지 않도록 보장
    - 과도한 요청으로 인한 법적/윤리적 문제 예방
    - “예의 있는(Polite)” 크롤러로서 신뢰성 확보
- **상호작용**
    - HTML 다운로더가 페이지를 요청하기 전, 해당 도메인의 `robots.txt`를 조회
    - 허용된 경로만 다운로드 진행
- **기술적 고려사항**
    - `robots.txt`는 주기적으로 **캐싱**하여 불필요한 반복 다운로드 방지
    - 사이트별로 규칙이 다르므로 파싱 로직 필요
    - 크롤러 사용자 에이전트(User-agent)에 따라 규칙 적용이 달라질 수 있음

```
User-agent: Googlebot
Disallow: /creatorhub/*
Disallow: /rss/people/*/reviews
```

### 3.3.2 성능 최적화 전략

- **역할**: HTML 다운로더가 대규모 웹 크롤링 환경에서 **최소 지연, 최대 처리량**을 달성하도록 성능을 최적화
- **왜 필요한가?**
    - 수십억 개 페이지를 빠르게 처리하려면 단일 서버·단일 요청 방식으로는 불가능
    - 네트워크 지연(DNS, 지역적 거리, 타임아웃 문제 등)을 최소화해야 대규모 분산 크롤링이 가능
- **상호작용**
    - 다운로더는 **분산 서버**와 협력하여 병렬 다운로드를 수행
    - DNS 변환, 서버 위치, 타임아웃 정책과 긴밀하게 연동
- **기술적 고려사항**
    1. **분산 크롤링**
        - 여러 서버와 스레드로 작업을 분산
        - URL 공간을 나누어 병렬 다운로드 수행
        - 부하 분산 및 처리 속도 향상
    2. **DNS 캐싱**
        - DNS 조회는 10~200ms 지연 발생 → 병목 원인
        - 결과를 캐싱하고 주기적으로 갱신해 성능 향상
    3. **지역성(Locality) 고려**
        - 크롤러 서버를 지역별로 분산 배치
        - 대상 서버와 물리적으로 가까울수록 네트워크 지연 최소화
    4. **짧은 타임아웃 설정**
        - 응답이 느리거나 없는 서버에 오래 묶이지 않도록 제한
        - 일정 시간 내 응답이 없으면 다운로드를 중단하고 다음 URL로 이동

### 3.3.3 안정성 확보 전략

- **역할**: HTML 다운로더가 장애나 오류 상황에서도 안정적으로 동작하도록 보장
- **왜 필요한가?**
    - 크롤링 대상은 방대하고 네트워크 오류가 빈번하게 발생
    - 단일 서버 장애가 전체 시스템 중단으로 이어지면 안 됨
    - 수집 데이터의 무결성과 지속성을 보장해야 함
- **상호작용**
    - 다운로더 서버 간 요청을 분산
    - 수집 상태와 데이터를 저장소에 기록하여 장애 시 복구 가능
    - 예외 처리 및 검증 모듈과 연동해 시스템 전반의 안정성 확보
- **기술적 고려사항**
    1. **Consistent Hashing**
        - 다운로더 서버 간 부하를 고르게 분산
        - 서버 추가/삭제 시 전체 재분배 없이 일부만 이동 → 확장성, 안정성 향상
    2. **상태 저장 & 장애 복구**
        - 크롤링 진행 상태와 데이터를 **지속성 저장소(persistent storage)**에 기록
        - 서버 장애 발생 시 저장된 상태를 불러와 이어서 실행 가능
    3. **예외 처리(Exception Handling)**
        - 대규모 환경에서는 연결 오류, 타임아웃, 잘못된 응답 등 예외가 빈번
        - 오류가 발생해도 전체 시스템이 멈추지 않고 **개별 요청 단위에서 복구 가능**해야 함
    4. **데이터 검증(Data Validation)**
        - 잘못된 데이터 저장을 방지하기 위해 수집된 콘텐츠를 검증
        - 무효한 HTML, 깨진 응답 등을 걸러내고 저장소 오염 방지

### 3.4 확장성 확보 전략

![image.png](https://github.com/user-attachments/assets/c90bb224-93e6-4e3f-984f-7a782aee820b)

- **역할**: 크롤러가 시간이 지나면서 등장하는 **새로운 콘텐츠 유형과 기능 요구**에 유연하게 대응할 수 있도록 지원
- **왜 필요한가?**
    - 웹은 끊임없이 진화하며 HTML뿐 아니라 이미지, 동영상, PDF, API 응답 등 다양한 형식을 포함
    - 처음부터 단일 구조로 설계하면 새로운 요구사항에 대응하기 어려움
    - 모듈형(플러그인 기반) 아키텍처로 설계해야 유지보수성과 확장성이 높아짐
- **상호작용**
    - 기본 크롤링 엔진은 공통 로직을 처리
    - 추가 모듈(다운로더, 분석기 등)이 플러그인 형태로 붙어 확장 기능 제공
- **기술적 고려사항**
    - 모듈 간 의존성 최소화 (독립 실행 가능 구조)
    - 새로운 모듈을 추가해도 기존 시스템 중단 없이 확장 가능
    - 자원 사용량(네트워크, 저장소)을 모듈 단위로 관리
- 예시 모듈
    - **PNG 다운로더**: 이미지 파일 다운로드 지원
    - **웹 모니터(Web Monitor)**: 저작권·상표권 침해 여부 모니터링

### 3.5 품질 저하 콘텐츠 회피 전략

- **역할**: 크롤링 과정에서 불필요하거나 잘못된 데이터를 수집하지 않도록 필터링
- **왜 필요한가?**
    - 중복·노이즈 콘텐츠는 저장 공간과 처리 자원을 낭비
    - 스파이더 트랩은 크롤러를 무한 루프에 빠뜨려 시스템 자원을 고갈시킬 수 있음
    - 품질 저하 콘텐츠를 사전에 걸러야 **저장 효율성과 검색 품질**이 보장됨
- **상호작용**
    - 콘텐츠 파서와 URL 필터 단계에서 품질 검증 수행
    - 중복/노이즈 검출 모듈과 크롤링 큐가 연동되어 불필요한 URL을 제거
- **기술적 고려사항**
    1. **중복 콘텐츠 감지**
        - 전체 웹 콘텐츠 중 약 **30%는 중복**
        - 해시(Hash), 체크섬(Checksum) 등을 활용해 빠른 판별
        - 저장소 오염 방지 및 처리 비용 절감
    2. **거미 덫(Spider Trap) 대응**
        - 크롤러를 무한 루프에 빠뜨리는 설계된 함정 페이지
        - 예: 무한히 이어지는 디렉터리 구조
        - 대응 방법
            - URL 길이 제한
            - 의심되는 패턴 수작업 탐지 후 **URL 필터**에 등록하여 제외
    3. **노이즈 콘텐츠 필터링**
        - 광고, 스팸ß URL, 무가치한 스크립트 등 **정보 가치가 낮은 데이터** 제외
        - 가능하다면 콘텐츠 분류기를 적용해 **검색 품질 최적화**

---

## 4단계: 마무리 및 심화 주제

### 4.1 요약

> 설계 인터뷰에서 강조할 키워드는 확장성, 안정성, 성능, 예절성 네 가지입니다.
> 
- **확장성 (Scalability)**
    - 분산 크롤링, 모듈형 구조, 디스크+메모리 하이브리드 저장소
- **안정성 (Robustness)**
    - Consistent Hashing, 상태 저장·장애 복구, 예외 처리·데이터 검증
- **성능 (Performance)**
    - DNS 캐싱, 지역성(Locality) 기반 서버 배치, 짧은 타임아웃 설정
- **예절성 (Politeness)**
    - 도메인별 큐, 요청 간 딜레이, robots.txt 준수

### 4.2 심화 주제 목록

> 추가로 논의할 수 있는 고급 설계 포인트들입니다.
> 
- **동적 렌더링 페이지 대응**
    - JS·AJAX 기반 콘텐츠 수집을 위한 렌더링 엔진 필요
- **스팸/저품질 콘텐츠 필터링**
    - 리소스 절약 및 크롤링 데이터 품질 개선
- **DB 샤딩 및 다중화**
    - 데이터 저장 계층의 확장성과 가용성 확보
- **수평 확장성 (Horizontal Scalability)**
    - 무상태(Stateless) 서버 구조 설계로 노드 확장 지원
- **CAP 이론 기반 고려**
    - 일관성, 가용성, 네트워크 분할 내성 간 트레이드오프 명확화
- **크롤링 분석 솔루션 도입**
    - 수집 현황, 성능, 품질을 모니터링하고 최적화