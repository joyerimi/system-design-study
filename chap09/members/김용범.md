# 9장 - 웹 크롤러 설계

## 들어가며

이번 9장은 `웹 크롤러 설계` 내용에 대해서 다룬다. 지금까지 크롤링을 Java 언어의 Jsoup 라이브러리를 통해서도 구현해 본 경험도 있고, Python 언어의 Selenium 라이브러리를 활용해서
동적 크롤링을 해본 경험이 있다. 그냥 라이브러리를 활용만 했을 뿐이지, 그것을 설계 측면에서는 한번도 생각해 본 적이 없었다. 이번 9장을 읽으면서 배경지식을 넓혀나가보자.

## 웹 크롤러 소개

웹 크롤러는 `로봇`, `스파이더`라고도 불리며, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다. 이러한 크롤러는 다양한 곳에서 이용된다.

1. 검색 엔진 인덱싱

- 크롤러의 가장 보편적인 용례다. 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.

2. 웹 아카이빙

- 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말한다. 많은 국립 도서관이 크롤러를 돌려 웹사이트를 아카이빙하고 있다.

3. 웹 마이닝

- 웹 마이닝을 통해서 인터넷에서 유용한 지식을 도출해낼 수 있다.

4. 웹 모니터링

- 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

## 1 단계 - 문제 이해 및 설계 범위 확정

### 웹 크롤러의 기본 알고리즘

웹 크롤러의 기본 알고리즘은 간단하다.

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

이렇게만 보면 웹 크롤러가 정말 단순해 보이지만, 절대 그렇지 않다. 엄청난 규모 확장성을 갖는 웹 크롤러를 설계하는 것은 엄청나게 어려운 작업이다. 따라서 설계를 진행하기 전에 질문을 던져서 요구사항을
알아내고 설계 범위를 좁히는 과정 또한 중요하다.

해당 장에서 제시한 설계 범위는 다음과 같다.

1. 용도: 검색엔진 인덱싱
2. 수집 빈도: 매달 10억 개의 웹 페이지 수집
    - QPS: 10억 / 30일 / 24시간 / 60분 / 60초 = 대략 400 페이지
    - 최대 Peak QPS = 2 x QPS = 800 페이지
    - 10억 페이지 x 500k(웹 페이지 크기 평균) = 500TB/월
    - 5년간 저장 -> 500TB x 12개월 x 5년 = 30PB 저장용량 필요
3. 특이사항: 새로 만들어진 웹 페이지나 수정된 웹 페이지 또한 고려
4. 저장 주기: 5년간 저장
5. 중복 처리: 중복된 컨텐츠를 갖는 페이지 무시

이러한 인터뷰어와의 대화 과정을 통해서 복잡하고 모호한 웹 크롤러 범위를 명확하게 만들 수 있다.

이 뿐만 아니라, 좋은 웹 크롤러가 만족시켜야 할 속성으로는 다음과 같은 것들이 있다.

1. 규모 확장성

- 웹은 거대하기 때문에 수십억 개의 페이지가 존재하는 것으로 알려져 있다. 따라서 병행성을 활용하면 보다 효과으로 웹 크롤링을 할 수 있을 것이다.

2. 안정성

- 웹은 함정으로 가득하다. 악성 코드가 붙어있는 링크를 가지는 웹, 잘못 작성된 HTML 등이 그 좋은 예시이다. 크롤러는 이런 비정상적 입력이나 환경에 잘 대응할 수 있어야 한다.

3. 예절

- 크롤러는 ㅈ수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안 된다. 그것은 민폐이자, 올바른 예절이 아니다.

4. 확장성

- 새로운 형태의 콘텐츠를 지원하기가 쉬워야 한다. 예를 들어, 이미지 파일도 크롤링하고 싶다고 했을 때, 이를 위해 전체 시스템을 새로 설계해야 한다면 곤란할 것이다.

## 2 단계 - 개략적 설계안 제시 및 동의 구하기

요구사항이 분명해지면 개략적 설계를 진행해보자.

<img width="1430" height="866" alt="Image" src="https://github.com/user-attachments/assets/5b95842a-01cc-4c43-9060-2668bb1b6bf3" />

위 다이어그램에 등장하는 컴포넌트 각각이 어떤 기능을 수행하는지 살펴보자.

### 시작 URL 집합

시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점이다. 예를 들어, 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은 모든
페이지의 URL을 시작 URL로 쓰는 것이다.

전체 웹을 크롤링해야하는 경우에는 시작 URL을 고를 때 좀 더 창의적인 필요가 있다. 그것은 바로 외대한 많은 링크를 탐색할 수 있도록 하기 위함이다. 그 창의성은 국가, 지역 또는 쇼핑, 스포츠, 건강과
같은 주제별로 다르게 적용될 수 있다. 그렇기 때문에 시작 URL로 무엇을 쓸 것이냐는 질문에 정답은 없다. 면접관도 완벽한 답안을 기대하는 것이 아니니 의도가 무엇인지만 정확히 전달하도록 하자.

### 미수집 URL 저장소

현대적 웹 크롤링 상태를 나눈다면 `다운로드할 URL`, `다운로드된 URL` 2가지로 나눌 수 있다. 여기서 미수집 URL 저장소라는 것은 다운로드할 URL로 `큐(Queue)`라고 생각하면 된다.

### HTML 다운로더

HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트이다. 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공한다.

### 도메인 이름 변환기 (URL -> IP)

웹 페이지를 다운받으려면 URL을 IP 주소로 변환해야한다. HTML 다운로더는 도메인 이름 변환기를 사용해서 URL에 대응되는 IP 주소를 알아낸다.
ex) www.wikipedia.org -> 198.35.26.96 (2019년 3월 5일 기준)

### 콘텐츠 파서

웹 페이지를 다운로드하면 `파싱`과 `검증` 절차를 거쳐야 한다. 이상한 웹 페이지는 문제를 일으킬 수 있는데다 저장 공간만 낭비하게 되기 때문이다. 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이
느려지게 될 수 있으므로, 독립된 컴포넌트로 따로 분리한다.

### 중복 콘텐츠 유무 판단

웹에 공개된 연구 결과에 따르면 29% 가량의 웹 페이지 콘텐츠는 중복이다. 따라서 같은 콘텐츠를 여러 번 저장하게 될 수 있다. 비교 대상 문서의 수가 방대할 경우, 두 HTML 문서를 비교하는 방법은
매우 비효율적이다. 가장 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것이다.

### 콘텐츠 저장소

콘텐츠 저장소는 HTML 문서를 보관하는 시스템이다. 저장소를 구현하는 데 쓰일 기술을 고를 때는 다음과 같은 사항을 유의해야한다.

1. 저장할 데이터의 유형
2. 저장할 데이터의 크기
3. 저장소 접근 빈도
4. 데이터의 유효 기간

이 4가지를 종합적으로 고려해야 한다. 본 설계안의 경우에는 디스크와 메모리를 동시에 사용하는 저장소를 택할 것이다.

- 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장한다.
- 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄일 것이다.

### URL 추출기

URL 추출기는 HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.

### URL 필터

URL 필터는 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 한다.

### 이미 방문한 URL

이미 방문한 적이 있는 URL인지 추적하면 같은 URL을 여러 번 처리하는 일을 방지할 수 있으므로 서버 부하를 줄이고, 시스템이 무한 루프에 빠지는 일을 방지할 수 있다. 해당 자료 구조로는 `블룸 필터`
또는 `해시 테이블`이 널리 쓰인다.

### URL 저장소

URL 저장소는 이미 방문한 URL을 보관하는 저장이다.

### 웹 크롤러 작업 흐름

1. 시작 URL 들을 미수집 URL 저장소에 저장한다.
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받는다.
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.
5. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시한다.
6. 중복 컨텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.
    - 이미 저장소에 있는 콘텐츠인 경우에는 처리하지 않고 버린다.
    - 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤, URL 추출기로 전달한다.
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다.
8. 골라낸 링크를 URL 필터로 전달한다.
9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다.
10. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살핀다. 이미 저장소에 있는 URL은 버린다.
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다.

## 3 단계 - 상세 설계

개략적 설계안을 토대로 이제 상세 설계로 들어가보도록 하자. 가장 중요한 컴포넌트와 그 구현 기술은 다음과 같다.

1. DFS vs BFS
2. 미수집 URL 저장소
3. HTML 다운로더
4. 안정성 확보 전략
5. 확장성 확보 전략
6. 문제 있는 콘텐츠 감지 및 회피 전략

### DFS vs BFS

웹은 유향 그래프이다. 페이지가 노드라고 하면, 하이퍼 링크는 에지라고 보면 된다. 웹 크롤링 프로세스를 DFS로 구현하게 된다면 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠하기 어려운 상황에
놓일 수 있다. 따라서 BFS(너비 우선 탐색)를 활용하면 해결할 수 있을 것 같지만, 이 또한 완전한 해결책은 아니다. 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다. 즉, 크롤러가 같은
호스트에 속한 많은 링크를 다운받는라 바빠지게 되는데, 이때 링크들을 병렬로 처리하게 된다면 과부하에 걸리게 될 것이다. 또한, 무한 루프에 빠질 수 있는 위험성도 존재한다. 이런 크롤러는 보통
`예의 없는 크롤러`로 간주한다.

### 미수집 URL 저장소

URL 저장소는 다운로드할 URL을 보관하는 장소이다. 이 저장소를 잘 구현하면 `예의를 갖춘 크롤러` 즉, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있다.

### 예의

웹 크롤러는 수집 대상 서버로 짧은 시간 안에 많은 요청을 보내는 것을 삼가야 한다. 너무 많은 요청을 보내는 것은 무례한 즉, 예의 없는 일이며, 때로는 `DoS` 공격으로 간주되기도 한다. 가령, 아무
안전장치가 없는 웹 크롤러의 경우, 초당 수천 건의 페이지 요청을 동일한 웹 사이트로 보내어 사이트를 마비시켜버릴 수도 있다.

### 예의 바른 크롤러

예의 바른 크롤러는 그럼 어떻게 구축이 가능할까? 예의 바른 크롤러를 만드는 데 있어서 지켜야 할 한 가지 원칙은 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것이다. 이러한 로직을 구현하기
위해서는 다음과 같은 컴포넌트들이 필요하다.

1. 큐 라우터

- 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할을 한다.

2. 매핑 테이블
    - 호스트 이름과 큐 사이의 관계를 보관하는 테이블
3. FIFO 큐
    - 같은 호스트에 속한 URL은 언제나 같은 큐에 보관한다.
4. 큐 선택기
    - 큐 선택기는 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할을 한다.
5. 작업 스레드
    - 작업 스레드는 전달된 URL을 다운로드하는 작업을 수행한다. 전달된 URL은 순차적으로 처리될 것이며, 작업들 사이에는 일정한 지연시간을 둘 수 있다.
6. 우선 순위
    - 유용성에 따라 URL의 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있을 것이다.
7. 순위 결정 장치
    - URL 우선 순위를 정하는 컴포넌트이다. 즉, URL을 입력으로 받아 우선순위를 계산한다.
8. 큐
    - 우선순위 별로 큐가 하나씩 할당된다. 우선순위가 높으면 선택될 확률도 올라간다.
9. 큐 선택기
    - 임의 큐에서 처리할 URL을 꺼내는 역할을 담당한다. 순위가 높은 큐에서 더 자주 꺼내도록 프로그램되어 있다.
10. 전면 큐

- 우선순위 결정 과정을 처리한다.

11. 후면 큐
    - 크롤러가 예의 바르게 동작하도록 보증한다.
12. 신선도
    - 웹 페이지는 수시로 추가되고, 삭제되고, 변경된다. 따라서 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집할 필요가 있다. 그러나 모든 URL을 재수집하는
      것은 많은 시간과 자원이 필요한 작업이다. 이 작업은 최적화하기 위한 전략으로는 다음과 같은 것들이 있다.
        1. 웹 페이지의 변경 이력 활용
        2. 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집

### 미수집 URL 저장소를 위한 지속성 저장장치

검색 엔진을 위한 크롤러의 경우, 처리해야 하는 URL의 수는 수억 개에 달한다. 그렇기 때문에 그 모두를 메모리에 보관하는 것은 안정성이나 규모 확장성 측면에서 바람직하지 않다. 전부 디스크에 저장하는
것도 좋은 방법은 아닌데, 느려서 쉽게 성능 병목지점이 될 수 있다. 따라서 대부분의 URL은 디스크에 두지만, IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두는 것이다. 버퍼에 있는 데이터는 주기적으로
디스크에 기록할 것이다.

### HTML 다운로더

HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다. 다운로더에 대해 알아보기 전에 먼저 `로봇 제외 프로토콜`을 살펴보아야 한다.

`Robots.txt` 파일(= 로봇 제외 프로토콜)은 웹사이트가 크롤러와 소통하는 표준적 방법이다. 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어 있다. 따라서 웹 사이트를 긁어 가기 전에
크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 한다. Robots.txt 파일을 반복적으로 다운로드하는 것을 피하기 위해서 이 파일은 주기적으로 다시 다운받아 캐시에 보관하는 것이 좋다.

### 성능 최적화

[추후 작성 예정]

## 4 단계 - 마무리

좋은 크롤러가 갖추어야 하는 특성들은 다음과 같다.

1. 규모 확장성
2. 예의
3. 확정성
4. 안정성

이러한 특성들을 모두 고려하여 규모 확장성이 뛰어난 웹 크롤러 설계 작업은 단순하지 않다. 시간이 허락한다면 면접관과 다음과 같은 것을 추가로 논의해보면 좋을 것 같다.

1. 서버 측 렌더링
    - 많은 웹사이트가 자바스크립트, AJAX 등의 기술을 사용해서 링크를 즉석에서 만들어 낸다. 그러니 웹 페이지를 그냥 있는 그대로 다운받아서 파싱해보면 그렇게 동적으로 생성되는 링크는 발견할 수
      없다. 이 문제를 해결하기 위해서는 페이지를 파싱하기 전에 서버 측 렌더링(동적 렌더링)을 적용하면 된다.
2. 원치 않는 페이지 필터링
    - 저장 공간 등 크롤링에 소요되는 자원은 유한하기 때문에 스팸 방지 컴포넌트를 두어 품질이 조악하거나 스팸성인 페이지를 걸러내도록 해 두면 좋다.
3. 데이터베이스 다중화 및 샤딩
4. 수평적 규모 확장성
5. 가용성, 일관성, 안정성
6. 데이터 분석 솔루션


