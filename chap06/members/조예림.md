![image.png](attachment:265b200d-4d6c-462d-b369-dd423a166b41:image.png)

### 0. 키-값 저장소 개요

- 키를 이용해 데이터를 빠르게 찾을 수 있는 단순한 데이터베이스

| 용어 | 설명 |
| --- | --- |
| **키(key)** | 데이터를 찾기 위한 유일한 식별자 (ex. 문자열, 해시값 등) |
| **값(value)** | 키와 연결된 실제 데이터 (ex. 문자열, 객체, 리스트 등) |
| **키-값 쌍(pair)** | 하나의 키와 하나의 값이 묶인 형태 (ex. 145 → "john") |
- 키는 **짧고 유일해야 성능이 좋다.**
- 값은 어떤 형태든 저장 가능 (문자열, 리스트, 객체 등).
- 일반적으로 **NoSQL** 시스템이며, **스키마가 없다**.

- 기본 연산
    - 저장: put(key, value)
    - 조회: get(key)
- 예시
    - Redis, Memcached, Amazon DynamoDB

---

### 1. 설계 목표 및 요구사항 정의

- **완벽한 설계는 없다.**
    - 읽기, 쓰기, 메모리 사용량 사이의 **균형 조정**이 필요
    - 일관성과 가용성 사이에서 **트레이드오프**를 고려한 설계가 핵심

### 주요 특성

- 기본적으로 키-값 쌍은 10KB 이하
- 큰 데이터도 저장할 수 있어야 함
- 장애 발생 시에도 빠른 응답 보장
- 트래픽에 따라 서버 자동 증설/감축 가능
- 필요에 따라 일관성 수준 조절 가능
- 빠른 응답 속도 제공

---

### 2. 단일 서버 키-값 저장소

### 2.1 기본 구조

- 한 대의 서버만 사용
- 모든 키-값 쌍을 해시 테이블 형태로 메모리에 저장
    - 매우 빠른 접근 속도 보장

### 2.2 단점 및 한계

| 문제점 | 설명 |
| --- | --- |
| **메모리 한계** | 데이터가 많아지면 모든 데이터를 메모리에 유지할 수 없음 |
| **확장성 없음** | 서버 1대로는 트래픽이나 저장 용량 한계에 부딪힘 |
| **장애에 취약** | 서버 1대에 의존하므로 다운되면 전체 시스템도 중단됨 |

### 2.3 개선 아이디어

- **데이터 압축(compression)** → 메모리 사용량 절약
- **핫 데이터만 메모리에** → 자주 쓰는 데이터만 캐싱, 나머지는 디스크에 저장

하지만 이런 방법들로도 **단일 서버 구조에는 한계**가 있음 

→ **분산 키-값 저장소(distributed key-value store)** 설계가 필요함

---

### 3. 분산 키-값 저장소

### 3.1 기본 개념

여러 대의 서버에 키-값 쌍을 나눠 저장하는 시스템

= **분산 해시 테이블 (Distributed Hash Table, DHT)**

### 3.2 CAP 이론 및 시스템 분류

![image.png](attachment:9d219001-f598-411e-8ea8-c21be15cb887:image.png)

분산 시스템은 일관성(Consistency), 가용성(Availability), 파티션 감내(Partition Tolerance)

이 세 가지 중 **동시에 세 가지를 모두 만족할 수 없다.**

→ **세 가지 중 두 가지를 선택하고, 하나는 포기해야 함.**

| 항목 | 설명 |
| --- | --- |
| **일관성 (Consistency)** | 모든 노드에서 **같은 데이터**를 읽을 수 있어야 함 |
| **가용성 (Availability)** | 일부 노드에 장애가 있어도 **항상 응답을 보장** |
| **파티션 감내성 (Partition Tolerance)** | 네트워크 통신 장애 발생 시에도 **시스템이 계속 작동**해야 함 |

### 시스템 분류

| 유형 | 설명 | 예시 |
| --- | --- | --- |
| **CP 시스템** | 일관성 + 파티션 감내 (가용성 ↓) | 장애 시 응답 지연 가능(ex. HBase) |
| **AP 시스템** | 가용성 + 파티션 감내 (일관성 ↓) | 장애 시 최신이 아닌 값 반환 허용(ex. DynamoDB, Cassandra) |
| **CA 시스템** | 일관성 + 가용성 (파티션 감내 ↓)  | **실제로 존재 불가** (네트워크 장애는 피할 수 없기 때문) |

> 실전에서는 CP 또는 AP 중 하나를 선택해야 하며,
> 
> 
> 대부분의 분산 키-값 저장소는 **AP 시스템을 선택**함
> 

---

### 4. 복제 기반 분산 저장소의 동작

### 4.1 이상적 동작 상태

![image.png](attachment:f3a46031-1671-4c08-baa6-dd83bbca3fd5:image.png)

- **구성**: 데이터가 `n1`, `n2`, `n3` 세 노드에 복제되어 저장됨
- **조건**: 네트워크에 파티션 없음 (모든 노드 정상 통신)
- **동작**: `n1`에 쓰인 데이터가 즉시 `n2`, `n3`로 복제됨
- **결과**
    - **일관성**: 모든 노드에서 동일한 데이터 제공
    - **가용성**: 어떤 노드에 요청해도 응답 가능

### 4.2 파티션 발생 시 동작

![image.png](attachment:c312bc1c-0e92-44d5-a8f3-169002b006ae:image.png)

- **현실에서는 파티션(통신 장애)을 피할 수 없음**
- 파티션 발생 시 **일관성 vs 가용성 중 하나를 선택**해야 함

### CP 시스템 (일관성 우선)

- n1과 n2에서 **쓰기 중단** → 데이터 불일치 방지
- **가용성 희생** (요청 거부 또는 오류 반환)
- 예: 은행 시스템

### AP 시스템 (가용성 우선)

- n1, n2에서 **쓰기/읽기 계속 허용**
- n3는 뒤늦게 동기화 → **일시적 불일치 허용**
- 예: SNS, 캐시 시스템

---

### 5. 시스템 컴포넌트 개요

- 주요 구성요소 8가지 소개
    1. **데이터 파티션**: 데이터를 여러 서버에 분산 저장
    2. **데이터 다중화 (Replication)**: 동일 데이터를 여러 노드에 복제해 장애 대비
    3. **일관성 (Consistency)**: 데이터 복제본 간 상태를 동일하게 유지
    4. **일관성 불일치 해소**: 데이터 충돌 발생 시 정합성 복원 방법
    5. **장애 처리**: 노드 장애 발생 시 시스템이 정상 동작을 유지하는 기술
    6. **시스템 아키텍처 다이어그램**: 전체 구조 시각화
    7. **쓰기 경로 (Write Path)**: 데이터가 저장될 때 흐름
    8. **읽기 경로 (Read Path)**: 데이터가 조회될 때 흐름

---

### 6. 데이터 파티션

### 목적

- 대규모 데이터를 **여러 서버에 나누어 저장**
- **단일 서버의 용량·부하 한계를 해결**하기 위함

### 설계 시 고려사항

1. **데이터가 서버에 고르게 분산되는가?**
2. **노드 추가/삭제 시 데이터 이동을 최소화할 수 있는가?**

### 해결 방법: **안정 해시(Consistent Hashing)**

![image.png](attachment:d1e7be94-bef8-4cc7-b1f0-5767f690b3a6:image.png)

- **서버와 키를 해시 링에 배치**
- 키는 링 상에서 시계 방향으로 가장 먼저 만나는 서버에 저장됨
- 예: `key0 → s1`에 저장

### 장점

- **자동 확장성**: 서버 추가/삭제 시에도 데이터 재배치가 최소화됨
- **이질성 지원**: 서버 성능에 따라 **가상 노드 개수 조절** 가능
    
    (고성능 서버에 더 많은 책임 부여)
    

---

### 7. 데이터 다중화 (Replication)

### 목적

- **가용성**과 **안정성** 확보를 위해 데이터 복제 필요
- 동일 데이터를 여러 서버에 **비동기적으로 저장**

### 방식

![image.png](attachment:c379ea0d-0d47-405a-baa7-b48b40ebb091:image.png)

- 복제본 개수 **N**은 조정 가능 (예: N = 3)
- **해시 링 상에서 키 위치 기준, 시계 방향으로 첫 N개 서버에 저장**
    - 예: `key0 → s1, s2, s3`

### 주의사항

- **가상 노드 사용 시**, 동일 물리 서버가 중복 선택될 수 있음
    
    → 복제 시 **같은 물리 서버 중복 저장은 피해야 함**
    

### 데이터 센터 고려

- **한 센터 내 복제만으로는 리스크 존재**
    - 정전, 네트워크 장애, 자연재해 등
- 따라서 **다른 데이터 센터에도 분산 복제** 필요
    
    → 센터 간은 **고속 네트워크**로 연결
    

---

### 8. 일관성 유지 전략

목적: 다중화된 데이터를 **동기화**하여 **일관성** 유지

### 8.1 정족수 기반 합의

- **N**: 데이터 복제본 개수
- **W**: 쓰기 성공으로 간주되기 위한 최소 응답 수
- **R**: 읽기 성공으로 간주되기 위한 최소 응답 수

> 예: N = 3, W = 1, R = 1 → 세 서버 중 하나만 응답해도 읽기/쓰기는 성공
> 

### 동작 방식

![image.png](attachment:89cc2fe3-1537-4c42-9ccf-31697621b391:image.png)

- **쓰기(W)**: 최소 W개 노드에서 **쓰기 성공 응답**을 받아야 완료
- **읽기(R)**: 최소 R개 노드에서 **읽기 응답**을 받아야 완료
- **중재자(coordinator)**: 클라이언트 대신 서버들과 통신하는 프록시 역할

### 8.2 일관성 vs 속도 트레이드오프

| 구성 | 특징 | 왜 그런가? |
| --- | --- | --- |
| **W = 1, R = N** | - 쓰기 빠름
- 읽기 느림
- 일관성 낮음 | 쓰기는 1곳에만 저장되면 끝나서 빠르지만, 읽기는 모든 노드 중 하나라도 오래된 데이터를 갖고 있으면 그걸 읽을 수 있음 |
| **W = N, R = 1** | - 읽기 빠름
- 쓰기 느림
- 일관성 낮음 | 모든 노드에 쓰기를 해야 하므로 느리지만, 어떤 노드에서 읽든 최신 데이터는 있음. 다만 네트워크 문제로 쓰기 중 일부 실패 가능성 있음 |
| **W + R > N** | - 읽기/쓰기 속도 적절
- **강한 일관성 보장** | 쓰기와 읽기 서버가 **적어도 하나 이상 겹치기 때문에**, 항상 최신 데이터를 읽는 노드가 포함됨
→ 예: N=3, W=2, R=2 → 겹치는 최소 1개 서버에서 최신 데이터 확인 가능 |
| **W + R ≤ N** | - 응답 빠름
- **약한 일관성** | 읽는 서버와 쓰는 서버가 겹치지 않을 수 있음 
→ 예: N=3, W=1, R=1 → 각각 다른 노드이면, 쓰기 반영 전 데이터를 읽을 수 있음 |

### 설계 시 기준

- **응답 속도**를 중시하면 W나 R 중 하나를 1로 설정
- **일관성**을 중시하면 W + R > N이 되도록 설정

---

### 9. 일관성 불일치 해소 전략

### 9.1 일관성 모델 종류

### 정의

데이터를 읽을 때 **얼마나 최신 상태를 보장할 것인지**를 결정하는 기준

### 주요 일관성 모델 종류

| 모델 | 설명 |
| --- | --- |
| **강한 일관성 (Strong Consistency)** | 모든 읽기 연산은 **가장 최근에 쓰인 값**을 반환→ 낡은 데이터를 절대 보지 않음 |
| **약한 일관성 (Weak Consistency)** | 읽기 연산이 최신값을 반환할 수도, 아닐 수도 있음 |
| **결과적 일관성 (Eventual Consistency)** | 일정 시간이 지나면 **모든 복제본이 결국 일치**→ 초기에는 다른 값이지만, 나중엔 동기화됨 |

### 강한 일관성의 특징

- 쓰기 결과가 **모든 복제본에 반영될 때까지 읽기/쓰기 차단**
- 최신성은 보장되지만, **응답 속도가 느리고 가용성이 낮음**
- **고가용성이 중요한 시스템에는 부적합**

### 결과적 일관성의 특징

- 쓰기 연산을 **빠르게 처리하고 나중에 복제본 간 동기화**
- 일시적으로 일관성이 깨질 수 있음
- **Dynamo, Cassandra** 등은 이 모델을 사용
- 일관성 깨짐을 막기 위해, **클라이언트 측에서 버전 관리 등 별도 처리 필요**

### 결론

- 키-값 저장소 설계 시, 대부분 **결과적 일관성 모델을 선택**
- **가용성과 성능을 우선시하되**, **일관성은 클라이언트 측에서 보완**
    
    **→ [비 일관성 해소 기법: 데이터 비저닝](https://www.notion.so/23273fc720b2805fb3e5f281c2f6bfba?pvs=21)** 
    

### 9.2 비 일관성 해소 기법

### 문제 배경: **데이터 충돌**

- 복제된 노드에서 **동시에 서로 다른 값으로 쓰기 발생** → 충돌
- 예: `n1 → "johnSanFrancisco"`, `n2 → "johnNewYork"` → 서로 다른 최신값 존재

### 해결책 ①: **버저닝 (Versioning)**

- 데이터에 **버전 정보를 부여**
- 변경 시마다 **새 버전 생성 (immutable)**
- 여러 버전이 동시에 존재할 수 있음 → **충돌 감지 및 해소 필요**

### 예시: 데이터 충돌 상황

- 데이터 `"name"`의 사본이 `n1`, `n2` 두 노드에 저장되어 있고,
    
    클라이언트 두 명(서버 1, 서버 2)이 동시에 값을 읽는다.
    
    ![image.png](attachment:9031a38b-6b1c-4a5e-ad22-e661312ba850:image.png)
    
- 이후, 두 서버가 서로 다른 값으로 동시에 `"name"`을 수정한다.
    
    ![image.png](attachment:560d878d-298c-4555-8e8e-16b69bf809da:image.png)
    
- 결과적으로
    - `n1`에는 `"johnSanFrancisco"` 저장
    - `n2`에는 `"johnNewYork"` 저장

→ 이렇게 **서로 다른 최신값 두 개가 생긴 것**이 바로 충돌 상태이며,

→ 각각의 값은 서로 다른 **버전(v1, v2)** 으로 간주된다.

### 해결책 ②: **벡터 시계 (Vector Clock)**

- 각 데이터에 `[서버, 버전번호]` 쌍을 붙임
    
    예: `D([S1, 1], [S2, 2])`
    
- 이를 통해 판단 가능
    - **어떤 버전이 이전 버전인지**
    - **서로 다른 경로에서 파생된 충돌 버전인지**

### 벡터 시계 동작 예

![image.png](attachment:b0c35a8f-d38c-43d2-a65d-e18e408ec570:image.png)

### 충돌 판별 방법

- **X가 Y의 이전 버전인지 확인**
    
    → Y의 모든 값이 X 이상이면 **선후 관계 있음** (충돌 아님)
    
- **충돌 여부 확인**
    
    → 서로 버전 값이 엇갈리면 **동시 파생 버전 → 충돌**
    
    예 1
    
    - `D([S0, 1], [S1, 1])`
    - `D([S0, 1], [S1, 2])`
        
        → 충돌 X
        
    - `D([S0, 1], [S1, 2])`
    - `D([S0, 2], [S1, 1])`
        
        → 서로 충돌 (누가 더 최신인지 불명확)
        

### 단점

1. **클라이언트가 충돌 해결 책임**
    
    → 구현 복잡도 증가
    
2. **버전 수 증가**
    
    → 순서쌍이 많아지면 성능 저하
    
    → 해결: 임계치 초과 시 오래된 항목 제거 (단, 정확도 감소) 
    
    → 실제 서비스에서는 문제 발생한 적 없다고 함
    

### 결론

- **벡터 시계는 실전에서도 사용 가능하고 효과적**
- **Dynamo, Cassandra** 등에서 채택된 방식
- 클라이언트가 일부 복잡해지지만, 고가용성 시스템에 적합한 충돌 감지/해결 메커니즘

---

### 10. 장애 처리 전략

### 장애 처리

- 대규모 시스템에서 **장애는 예외가 아니라 일상적인 사건**
- 따라서 장애 발생을 **빨리 감지하고**, **정확히 복구하는 설계**가 중요

### 장애 처리 구성

1. **장애 감지 (Failure Detection)**
    - 노드, 네트워크, 서비스의 장애를 빠르고 정확하게 알아내는 기술
2. **장애 해소 (Failure Resolution)**
    - 장애가 발생했을 때 데이터를 복구하거나, 정상 노드로 대체하는 전략

### 10.1 장애 감지 방식

### 기본 원칙

- **한 노드만의 판단으로 장애 처리하지 않음**
- **두 개 이상의 노드가 동일하게 보고**해야 장애로 간주

### 방법 ①: 멀티캐스팅

![image.png](attachment:5878e56a-da4a-46f1-880f-ceab645856ae:image.png)

- 모든 노드가 서로에게 장애 여부를 직접 알림
- **단순하지만**, 노드 수가 많아지면 **비효율적**

### 방법 ②: **가십 프로토콜 (Gossip Protocol)**

→ **분산형 장애 감지 방식**, 확장성과 효율성 모두 갖춤

### 동작 방식

1. 각 노드는 **멤버십 목록**을 유지
    - `[노드 ID, 하트비트 카운터]` 목록
2. 주기적으로 **자신의 하트비트 카운터 증가**
3. 무작위 노드에게 **멤버십 목록 전파**
4. 받은 노드는 **자신 목록과 비교하여 최신화**
5. 특정 노드의 하트비트가 **오랫동안 갱신되지 않으면 장애로 간주**

### 예시

![image.png](attachment:739a5a68-dd81-4267-a38d-a5fc1fd6d649:image.png)

- 노드 `s0`이 `s2`의 하트비트가 오래 변하지 않음을 감지
- 이를 다른 노드에 전달 → **다수 노드가 동일하게 인식하면 장애로 처리**

### 결론

- **멀티캐스트 방식**은 단순하지만 확장성 낮음
- **가십 프로토콜**은 분산 환경에 적합한 **효율적 장애 감지 방식**

### 10.2 일시적 장애 처리

### 문제

- 가십 프로토콜로 **노드 장애 감지**
- 그런데 **정족수(quorum)** 접근을 그대로 적용하면
    
    → 장애 시 **읽기/쓰기를 차단해야 함** → 가용성 저하
    

### 해결: **느슨한 정족수 (Sloppy Quorum)**

- 장애 노드를 제외하고 **정상 동작 중인 노드들로만 W, R 만족**
- 해시 링상 원래 대상이 아니더라도 **대체 서버가 임시로 연산을 수행**

### 보완 기법: **단서 후 임시 위탁 (Hinted Handoff)**

- 임시로 연산을 처리한 서버는, 원래 처리해야 했던 장애 노드에게 **나중에 변경사항을 전달**
- 이때 남겨두는 메타정보를 **Hint**라고 부름

### 예시

![image.png](attachment:1e4ae651-988c-4ae0-b662-869ed5645b53:image.png)

- `s2`가 장애 상태면 `s3`가 대신 읽기/쓰기를 처리
- `s2`가 복구되면, `s3`가 **최신 데이터 넘겨줌**

| 기법 | 역할 |
| --- | --- |
| **Sloppy Quorum** | 장애 노드를 제외하고도 W/R 보장 → 가용성 유지 |
| **Hinted Handoff** | 나중에 원래 노드가 복구되면 데이터 동기화 |

→ **가용성을 유지하면서도, 복구 후 일관성 회복 가능**

### 10.3 영구 장애 처리

### 문제

- 일시적 장애는 **hinted handoff**로 처리 가능
- 하지만 **영구 장애**(복구되지 않는 노드)는 **복제본 간 데이터 불일치**로 이어짐

### 해결: **반-엔트로피 프로토콜 (Anti-Entropy Protocol)**

- **복제본끼리 주기적으로 데이터를 비교 및 동기화**
- 최신 버전으로 데이터 갱신

### 최적화 도구: **머클 트리 (Merkle Tree)**

- 해시 트리의 일종
- 각 노드는 자식 노드들의 **해시값으로 구성**
- 루트 노드만 비교해도 전체 데이터가 **동기화되었는지 빠르게 확인 가능**

### 동작

1. 키 공간을 **버킷** 단위로 나눔
    
    ![image.png](attachment:f2f2dc98-5ee2-4a3b-9fc3-d04734b74bb3:image.png)
    
2. 각 키에 **해시 함수 적용**
    
    ![image.png](attachment:3ee32148-7b02-457d-9c98-d6259a3ca68f:image.png)
    
3. 해시값으로 하위 노드 구성 → 상향식으로 트리 구성
    
    ![image.png](attachment:3430faa6-f067-495d-b585-69aaa5c69ab3:image.png)
    
    ![image.png](attachment:0c951a11-7c12-44f9-95bc-9a4d1690a037:image.png)
    
4. **루트 해시값 비교**
    
    ![image.png](attachment:9527a4e4-b315-42ef-8668-5bda0df3017a:image.png)
    
    - 같으면 데이터 동일
    - 다르면 아래로 내려가며 **차이 나는 버킷만 동기화**

### 장점

- 동기화 대상이 되는 데이터 양을 **차이 나는 부분에만 국한**
- 데이터 전체 양과 무관하게 **효율적인 비교·복구 가능**

### 단점

- 버킷 하나에 많은 키가 들어가면, 버킷 단위로만 비교하므로 **미세한 차이도 전체 버킷 동기화 필요**
- 머클 트리는 **무엇이 최신인지 판단하지 못함**
    
    → 충돌 버전이 있으면 별도 버전 정보(예: 벡터 시계)가 필요
    
- 트리 구조와 해시값 저장으로 인해 **추가 자원 소비** 발생
- 변화가 적더라도 **정기적으로 비교 실행** → 네트워크·CPU 사용 증가

| 항목 | 내용 |
| --- | --- |
| **반-엔트로피** | 영구 장애 시 복제본 간 **데이터 불일치 복구** 프로토콜 |
| **머클 트리** | 차이 나는 데이터만 찾아내어 **효율적 동기화** 수행 |

### 10.4 데이터 센터 장애 대응

### 문제

- 정전, 네트워크 장애, 자연재해 등으로 **전체 데이터 센터가 마비될 수 있음**

### 해결

- **데이터를 여러 데이터 센터에 다중화(geo-replication)**
- 하나의 센터가 완전히 중단돼도 **다른 센터에서 서비스 지속 가능**

### 설계 포인트

- 데이터 센터 간은 **고속 네트워크로 연결**
- **복제 정책**과 **일관성 수준**을 유연하게 조절해야 함
    
    (ex. 동기 vs 비동기 복제)
    

---

### 11. 시스템 아키텍처

### 11.1 구성 요소

![image.png](attachment:d69b3b53-0d37-4396-8249-212b90e4b31b:image.png)

1. **클라이언트**
    - `get(key)`, `put(key, value)` API 호출
2. **중재자(Coordinator)**
    - 클라이언트 요청을 받아 적절한 노드에 위임
    - **프록시 역할**
3. **노드(Node)**
    - **안정 해시(Consistent Hash)** 링에 분산 배치
    - 데이터 저장 및 다중화 담당
    - 클러스터 내에서 **동등한 역할 수행**

### 11.2 시스템 특성

| 특성 | 설명 |
| --- | --- |
| **완전 분산 구조** | 노드 추가/제거 자유로움, SPOF 없음 |
| **데이터 다중화** | 고가용성과 장애 대응 지원 |
| **모든 노드가 동등** | 동일 기능 수행 (클라이언트 API, 장애 감지, 복구, 충돌 해소 등) |

### 11.3 노드 내부 구조

![image.png](attachment:1bd69547-4748-445a-9977-c171467954b4:image.png)

시스템 전체는 **확장성, 고가용성, 무중단 장애 대응**을 목표로 설계되어 있음.

---

### 12. 쓰기 경로 (Write Path)

### 참고 모델: Cassandra 구조 기반

### 동작 순서

![image.png](attachment:6f621bd2-5e13-491d-a184-c787b1270829:image.png)

| 단계 | 저장 위치 | 역할 |
| --- | --- | --- |
| 1 | Commit Log (디스크) | 장애 복구용 |
| 2 | MemTable (메모리) | 빠른 쓰기 처리 |
| 3 | SSTable (디스크) | 영구 저장, 읽기 최적화 구조 |
1. **커밋 로그 기록**
    - 클라이언트의 쓰기 요청을 **디스크의 커밋 로그에 먼저 기록**
    - 장애 발생 시 데이터 복구용
2. **메모리 캐시 기록 (MemTable)**
    - 요청 데이터를 **메모리상에 저장**
    - 빠른 접근 및 임시 저장 공간 역할
3. **디스크 저장 (Flush → SSTable)**
    - 메모리 캐시가 **가득 차거나 임계치 도달 시 디스크로 플러시**
    - 디스크에는 **정렬된 형태(SSTable)**로 저장
    - SSTable: **Sorted String Table**, 키-값 쌍을 정렬하여 유지

---

### 13. 읽기 경로 (Read Path)

![image.png](attachment:e9862e65-fa64-41e0-8b15-1c647f820b6b:image.png)

![image.png](attachment:88c6429d-343a-4958-89c5-195ad2c710b2:image.png)

| 단계 | 동작 |
| --- | --- |
| 1 | 메모리 캐시에서 조회 |
| 2 | 없으면 블룸 필터로 SSTable 후보 결정 |
| 3 | 디스크에서 SSTable 읽기 |
| 4 | 결과 반환 |

### 메모리 캐시 우선 조회

- **읽기 요청**이 들어오면, 노드는 먼저 **메모리 캐시(MemTable)**를 확인
- **데이터가 있으면** → 즉시 클라이언트에게 반환 (빠른 응답)

### 메모리에 없을 경우 → 디스크 조회

1. **메모리 캐시에 데이터 없음**
2. **블룸 필터(Bloom Filter)**로 어떤 SSTable에 키가 있을 가능성이 있는지 확인
3. 블룸 필터가 가리키는 **SSTable에서 디스크 조회**
4. **찾은 데이터를 반환**

### 블룸 필터란?

- 공간 효율적인 **확률 기반 자료 구조**
- 특정 키가 **존재할 가능성이 있는 SSTable만 탐색**하게 도와줌
    
    → 디스크 접근 횟수 최소화